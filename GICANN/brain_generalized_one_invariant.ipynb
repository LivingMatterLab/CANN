{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se438kR0HIri"
      },
      "source": [
        "# Automated Model Discovery for Brain Tissue\n",
        "\n",
        "Model Discovery Papers:\n",
        "1. Invariant-based: https://www.sciencedirect.com/science/article/pii/S1742706123000661\n",
        "2. Principal-stretch-based: https://www.sciencedirect.com/science/article/pii/S2666522023000047\n",
        "\n",
        "Brain Data Reference: https://www.sciencedirect.com/science/article/pii/S1742706116305633\n",
        "\n",
        "Code by Denisa Martonová\n",
        "Last edited August 2025\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc5wZXS329As"
      },
      "source": [
        "\n",
        "\n",
        "### 0. Load python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ODWLtwLAQQD"
      },
      "outputs": [],
      "source": [
        "# matplotlib 3.7 and above removed key plotting features used in this notebook; tensorflow is starting to move some aspects to legacy in 2.13.0 but it will run still\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koxfCXBQn_9g",
        "outputId": "7c5e8243-c006-49ee-d415-6f038cc41fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy: 1.23.5\n",
            "Matplotlib: 3.2.2\n",
            "Tensorflow: 2.12.0\n"
          ]
        }
      ],
      "source": [
        "# import necessary python packages\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.backend as K\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Check Versions\n",
        "print('Numpy: ' + np.__version__)\n",
        "print('Matplotlib: ' + matplotlib.__version__) # must be 3.2.2\n",
        "print('Tensorflow: ' + tf.__version__)\n",
        "#print('Keras: ' + keras.__version__) # comment out if using tf 2.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGsjGLS7zi5A",
        "outputId": "2107c78d-4961-4ce3-f608-0bd60d84cc5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Import excel file, change to match where you saved the file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/CANN_Stanford/generalized_invariants/' # change to where you download this; must be in Google Drive\n",
        "dfs = pd.read_excel(path + 'input/CANNsBRAINdata.xlsx', sheet_name='Sheet1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuvDD-UW3UIa"
      },
      "source": [
        "### 1. Load Brain data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6IYNvlxqvQQ"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "def getStressStrain(Region):\n",
        "    if Region =='brain':\n",
        "        if Region =='brain':\n",
        "\n",
        "            lam1_uc= dfs.iloc[3:20,0].dropna().astype(np.float64).values\n",
        "            T1_uc = dfs.iloc[3:20,1].dropna().astype(np.float64).values*lam1_uc\n",
        "\n",
        "            lam1_ut= dfs.iloc[19:36,0].dropna().astype(np.float64).values\n",
        "            T1_ut = dfs.iloc[19:36,1].dropna().astype(np.float64).values*lam1_ut\n",
        "\n",
        "            #exclude simple shear for training\n",
        "            lam_ss = tf.zeros_like(lam1_uc)\n",
        "            T1_ss = tf.zeros_like(T1_uc)\n",
        "    return lam1_ut,T1_ut,lam1_uc,T1_uc, lam_ss, T1_ss\n",
        "\n",
        "# Define different loading protocols\n",
        "def traindata(modelFit_mode):\n",
        "    if modelFit_mode == 'bi':\n",
        "        model_given = model_bi\n",
        "        input_train = [lam1,lam2]\n",
        "        output_train = [T1_bi,T2_bi]\n",
        "\n",
        "\n",
        "    elif modelFit_mode == \"uni\":\n",
        "        model_given = model_uni\n",
        "        input_train = lam_uni\n",
        "        output_train = T1_uni\n",
        "\n",
        "    elif modelFit_mode == \"combi\":\n",
        "        model_given = model_combi\n",
        "        input_train = [lam1_uc,lam1_ut,lam_ss]\n",
        "        output_train = [T1_uc,T1_ut,T1_ss]\n",
        "\n",
        "    return model_given, input_train, output_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdsxsk3V1cZp"
      },
      "source": [
        "### L1 and L2 regularization with penalty weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDqcHT6r1a8J"
      },
      "outputs": [],
      "source": [
        "def regularize(reg, pen):\n",
        "    if reg == 'L2':\n",
        "        return keras.regularizers.l2(pen)\n",
        "    if reg == 'L1':\n",
        "        return keras.regularizers.l1(pen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YJz85Qq1rVu"
      },
      "source": [
        "## 2a. Strain Energy Model - Invariant-based\n",
        "\n",
        "\n",
        "Next, we define the strain energy function for our isotropic, perfectly incompressible Constitutive Artificial Neural Network with two hidden layers and four and twelve nodes using the invariants of the right Cauchy Green tensor. The first layer generates powers $(\\circ)^1$ and $(\\circ)^2$ of the network inputs,\n",
        "$[I_1-3]$ and $[I_2-3]$, and the second layer applies the identity, $(\\circ)$, the exponential function, $(\\rm{exp}((\\circ))-1)$, and the natural logarithm, $(-\\rm{ln}(1-(\\circ)))$, to these powers.\n",
        "The set of equations for this networks takes the following explicit form,\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{array}{l@{\\hspace*{0.1cm}}c@{\\hspace*{0.1cm}}\n",
        "              l@{\\hspace*{.02cm}}l@{\\hspace*{0.1cm}}\n",
        "              l@{\\hspace*{0.1cm}}c@{\\hspace*{0.1cm}}\n",
        "              l@{\\hspace*{0.1cm}}l@{\\hspace*{0.1cm}}l@{\\hspace*{0.04cm}}\n",
        "              l@{\\hspace*{0.1cm}}c@{\\hspace*{0.1cm}}\n",
        "              l@{\\hspace*{0.1cm}}l@{\\hspace*{0.1cm}}l@{\\hspace*{0.0cm}}\n",
        "              l@{\\hspace*{0.1cm}}l@{\\hspace*{0.1cm}}c@{\\hspace*{0.1cm}}\n",
        "              l@{\\hspace*{0.1cm}}l@{\\hspace*{0.1cm}}l@{\\hspace*{0.0cm}}l}\n",
        "    \\psi(I_1,I_2)\n",
        "&=& w_{2,1}&w_{1,1} &[\\,I_1 - 3\\,]\n",
        "&+& w_{2,2} & [ \\, \\exp (\\,     w_{1,2} & [\\, I_1 -3 \\,]&)   - 1\\,]\n",
        "&-& w_{2,3} &      \\ln (\\, 1 -  w_{1,3} & [\\, I_1 -3 \\,]&) \\\\\n",
        "&+& w_{2,4}&w_{1,4} &[\\,I_1 - 3\\,]^2\n",
        "&+& w_{2,5} & [ \\, \\exp (\\,     w_{1,5} & [\\, I_1 -3 \\,]^2&) - 1\\,]\n",
        "&-& w_{2,6} &      \\ln (\\, 1 -  w_{1,6} & [\\, I_1 -3 \\,]^2&) \\\\\n",
        "&+& w_{2,7}&w_{1,7} &[\\,I_2 - 3\\,]\n",
        "&+& w_{2,8} & [ \\, \\exp (\\,     w_{1,8} & [\\, I_2 -3 \\,]&)   - 1\\,]\n",
        "&-& w_{2,9} &      \\ln (\\, 1 -  w_{1,9} & [\\, I_2 -3 \\,]&)\\\\\n",
        "&+& w_{2,10}&w_{1,10} &[\\,I_2 - 3\\,]^2\n",
        "&+& w_{2,11} & [ \\, \\exp (\\,    w_{1,11} & [\\, I_2 -3 \\,]^2&)- 1\\,]\n",
        "&-& w_{2,12} &      \\ln (\\, 1 - w_{1,12} & [\\, I_2 -3 \\,]^2&) \\,.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "\n",
        "First we define the activation functions and a single Invariant block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jbyl-Tt2vJt"
      },
      "outputs": [],
      "source": [
        "initializer_exp = tf.keras.initializers.RandomUniform(minval=0., maxval=0.1, seed=np.random.randint(0,10000)) # use random integer as seed\n",
        "initializer_1 = 'glorot_normal'\n",
        "\n",
        "# Self defined activation functions for exp term\n",
        "def activation_Exp(x):\n",
        "    return 1.0*(tf.math.exp(x) -1.0)\n",
        "# Self defined activation functions for ln term\n",
        "def activation_ln(x):\n",
        "    return -1.0*tf.math.log(1.0 - (x))\n",
        "\n",
        "# Define network block\n",
        "## kernel is weight\n",
        "def SingleInvNet(I1_ref, idi, reg, pen):\n",
        "    # input: invariant\n",
        "    I_1_w11 = keras.layers.Dense(1,kernel_initializer=initializer_1,kernel_constraint=keras.constraints.NonNeg(),\n",
        "                                 kernel_regularizer=regularize(reg, pen),\n",
        "                                 use_bias=False, activation=None,name='w'+str(1+idi)+'1')(I1_ref) # no activation, correspont to the top ptahway in graph\n",
        "    I_1_w21 = keras.layers.Dense(1,kernel_initializer=initializer_exp,kernel_constraint=keras.constraints.NonNeg(),\n",
        "                                 kernel_regularizer=regularize(reg, pen),\n",
        "                                 use_bias=False, activation=activation_Exp,name='w'+str(2+idi)+'1')(I1_ref) # exp activation, 2nd line in graph\n",
        "\n",
        "    I_1_w31 = keras.layers.Dense(1,kernel_initializer=initializer_1,kernel_constraint=keras.constraints.NonNeg(),\n",
        "                                 kernel_regularizer=regularize(reg, pen),\n",
        "                              use_bias=False, activation=activation_ln,name='w'+str(3+idi)+'1')(I1_ref) # ln activation\n",
        "\n",
        "    collect = [I_1_w11, I_1_w21,I_1_w31]\n",
        "    collect_out = tf.keras.layers.concatenate(collect, axis=1)\n",
        "\n",
        "    return collect_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjNgzkY63fQ7"
      },
      "source": [
        "Then we define the strain energy keras submodel as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wDqgN2F1m4l"
      },
      "outputs": [],
      "source": [
        "def StrainEnergyCANN_invariant(reg, pen):\n",
        "\n",
        "    # Inputs defined\n",
        "    I1_in = tf.keras.Input(shape=(1,), name='I1')\n",
        "\n",
        "    # Put invariants in the reference configuration (substrct 3)\n",
        "    I1_ref = keras.layers.Lambda(lambda x: (x-3.0))(I1_in)\n",
        "\n",
        "    I1_out = SingleInvNet(I1_ref, 0, reg, pen)\n",
        "    terms = I1_out.get_shape().as_list()[1] #\n",
        "\n",
        "    ALL_I_out = [I1_out]\n",
        "    ALL_I_out = tf.keras.layers.concatenate(ALL_I_out,axis=1)\n",
        "\n",
        "    # second layer\n",
        "    ## get 2nd col of weights\n",
        "    W_ANN = keras.layers.Dense(1,kernel_initializer='glorot_normal',kernel_constraint=keras.constraints.NonNeg(),\n",
        "                               kernel_regularizer=regularize(reg, pen),\n",
        "                           use_bias=False, activation=None,name='wx2')(ALL_I_out)\n",
        "    Psi_model = keras.models.Model(inputs=[I1_in], outputs=[W_ANN], name='Psi')\n",
        "\n",
        "    return Psi_model, terms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm8k_7Ve3yAb"
      },
      "source": [
        "### 3. Stress Models\n",
        "\n",
        "\n",
        "####  Tension and compression\n",
        "\n",
        "For the case of uniaxial tension and compression, we stretch the specimen in one direction,\n",
        "$F_{11} = \\lambda_1 = \\lambda$.\n",
        "For an isotropic, perfectly incompressible material with\n",
        "$I_3 = \\lambda_1^2  \\lambda_2^2  \\lambda_3^2 = 1$,\n",
        "the stretches orthogonal to the loading direction are identical and equal to the square root of the stretch,\n",
        "$F_{22} = \\lambda_2 = \\lambda^{-1/2}$ and\n",
        "$F_{33} = \\lambda_3 = \\lambda^{-1/2}$.\n",
        "From the resulting deformation gradient,\n",
        "$F= {\\rm{diag}} \\, \\{ \\; \\lambda, \\lambda^{-1/2}, \\lambda^{-1/2} \\,\\}$,\n",
        "we calculate the first and second invariants and their derivatives,\n",
        "\n",
        "$$\n",
        "  I_1\n",
        "= \\lambda^2 + \\frac{2}{\\lambda}\n",
        "  \\quad \\mbox{and} \\quad\n",
        "  I_2\n",
        "= 2\\lambda + \\frac{1}{\\lambda^2}\n",
        "  \\quad \\mbox{with} \\quad\n",
        "  \\frac{\\partial I_1}{\\partial  \\lambda}\n",
        "= 2 \\, \\left[\\lambda - \\frac{1}{\\lambda^2} \\right]\n",
        "  \\quad \\mbox{and} \\quad\n",
        "  \\frac{\\partial I_2}{\\partial  \\lambda}\n",
        "= 2 \\, \\left[1 - \\frac{1}{\\lambda^3}\\right] \\,,\n",
        "$$\n",
        "\n",
        "to evaluate the nominal uniaxial stress $P_{11}$\n",
        "using the general stress-stretch relationship for perfectly incompressible materials,\n",
        "$ P_{ii}\n",
        "= [{\\partial \\psi}/{\\partial I_1}] \\,\n",
        "  [{\\partial I_1} /{\\partial \\lambda_i}]\n",
        "+ [{\\partial \\psi}/{\\partial I_2}] \\,\n",
        "  [{\\partial I_2} /{\\partial \\lambda_i}]\n",
        "- [{1}/{\\lambda_i}] \\, p $,\n",
        "for $i = 1,2,3$.\n",
        "Here, $p$ denotes the hydrostatic pressure that we determine from the zero stress condition in the transverse directions, $P_{22} = 0$ and $P_{33} = 0$, as\n",
        "$ p\n",
        "= [{2}/{\\lambda}] \\;\n",
        "  {\\partial \\psi}/{\\partial I_1}\n",
        "+ [2\\lambda+{2}/{\\lambda^2}] \\,\n",
        "  {\\partial \\psi}/{\\partial I_2}$.\n",
        "This results in the following explicit uniaxial stress-stretch relation for perfectly incompressible, isotropic materials,\n",
        "\n",
        "$$\n",
        "  P_{11}\n",
        "= 2 \\,\n",
        "  \\left[\n",
        "  \\frac{\\partial \\psi}{\\partial I_1}\n",
        "+ \\frac{1}{\\lambda}\n",
        "  \\frac{\\partial \\psi}{\\partial I_2}\n",
        "  \\right]\n",
        "  \\left[\n",
        "  \\lambda - \\frac{1}{\\lambda^2}\n",
        "  \\right]\\,.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY2kbH8d357e"
      },
      "source": [
        "####  Shear\n",
        "\n",
        "For the case of simple shear, we shear the specimen in one direction, $F_{12} = \\gamma$.\n",
        "For an isotropic, perfectly incompressible material with\n",
        "$F_{11} = F_{22} = F_{33} = 1$,\n",
        "we calculate the first and second invariants and their derivatives,\n",
        "$$\n",
        "  I_1\n",
        "= 3 + \\gamma^2\n",
        "  \\quad \\mbox{and} \\quad\n",
        "  I_2\n",
        "= 3 + \\gamma^2\n",
        "  \\quad \\mbox{with} \\quad\n",
        "  \\frac{\\partial I_1}{\\partial  \\lambda}\n",
        "= 2 \\, \\gamma\n",
        "  \\quad \\mbox{and} \\quad\n",
        "  \\frac{\\partial I_2}{\\partial  \\lambda}\n",
        "= 2 \\, \\gamma \\,,\n",
        "$$\n",
        "to evalute the nominal shear stress $P_{12}$\n",
        "using the general stress-stretch relationship for perfectly incompressible materials.\n",
        "This results in the following explicit shear stress-strain relation for perfectly incompressible, isotropic materials,\n",
        "$$\n",
        "  P_{12}\n",
        "= 2\\,\n",
        "  \\left[\n",
        "  \\frac{\\partial \\psi}{\\partial I_1}\n",
        "+ \\frac{\\partial \\psi}{\\partial I_2}\n",
        "  \\right]\n",
        "  \\gamma\\,.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVH0ZVdl5gUu"
      },
      "outputs": [],
      "source": [
        " # uniaxial\n",
        "def Stress_cal_uni_11(inputs):\n",
        "    (dPsidJ1,dJ1dF1,minus) = inputs\n",
        "    T1 = dPsidJ1*(dJ1dF1 - minus)\n",
        "    return T1\n",
        "\n",
        "def Stress_cal_uni_11_I1I2(inputs):\n",
        "    (dPsidJ1,dPsidJ2,dJ1dF1,minus1,dJ2dF1,minus2) = inputs\n",
        "    T1 = (dPsidJ1*(dJ1dF1 - minus1)+ dPsidJ2*(dJ2dF1 - minus2))\n",
        "    return T1\n",
        "\n",
        "    # biaxial\n",
        "def Stress_cal_bi_11(inputs):\n",
        "    (dPsidJ1,dJ1dF1,minus1) = inputs\n",
        "    two = tf.constant(2.0,dtype='float32')\n",
        "    one = tf.constant(1.0,dtype='float32')\n",
        "    T1 = (dPsidJ1*(dJ1dF1 - minus1))\n",
        "    return T1\n",
        "\n",
        "def Stress_cal_bi_22(inputs):\n",
        "    (dPsidJ2,dJ1dF2,minus1) = inputs\n",
        "    two = tf.constant(2.0,dtype='float32')\n",
        "    one = tf.constant(1.0,dtype='float32')\n",
        "    T2 = (dPsidJ2*(dJ1dF2 - minus1))\n",
        "    return T2\n",
        "\n",
        " # biaxial\n",
        "def Stress_cal_bi_11_I1I2(inputs):\n",
        "    (dPsidJ1,dPsidJ2,dJ1dF1,minus1,dJ2dF1,minus2) = inputs\n",
        "    two = tf.constant(2.0,dtype='float32')\n",
        "    one = tf.constant(1.0,dtype='float32')\n",
        "    T1 = (dPsidJ1*(dJ1dF1 - minus1)+ dPsidJ2*(dJ2dF1 - minus2))\n",
        "    return T1\n",
        "def Stress_cal_bi_22_I1I2(inputs):\n",
        "    (dPsidJ1,dPsidJ2,dJ1dF2,minus1,dJ2dF2,minus2) = inputs\n",
        "    two = tf.constant(2.0,dtype='float32')\n",
        "    one = tf.constant(1.0,dtype='float32')\n",
        "    T2= (dPsidJ1*(dJ1dF2 - minus1) + dPsidJ2*(dJ2dF2 - minus2))\n",
        "    return T2\n",
        "    #shear\n",
        "\n",
        "def Stress_cal_ss(inputs):\n",
        "    (dPsidI1, dJalpha) = inputs\n",
        "    two = tf.constant(2.0,dtype='float32')\n",
        "    stress = dPsidI1*dJalpha\n",
        "    return stress\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTwDkRFm4BWf"
      },
      "source": [
        "Finally, we can define seperate stress models for tension/compression, shear and a combination of all loading states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLlSaN-P4EHU"
      },
      "outputs": [],
      "source": [
        "# Gradient function\n",
        "## automatic gradients\n",
        "def myGradient(a, b):\n",
        "    der = tf.gradients(a, b, unconnected_gradients='zero')\n",
        "    return der[0]\n",
        "# Define H-layer\n",
        "class H_Layer_generalized_combi(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, nameU, setAl, init):\n",
        "        super(H_Layer_generalized_combi, self).__init__()\n",
        "        self.nameU = nameU\n",
        "        self.setAl = setAl\n",
        "        self.init =  init\n",
        "\n",
        "        self.alpha =  tf.Variable(initial_value=self.init, name=self.nameU, constraint=keras.constraints.NonNeg(), dtype=tf.float32, trainable=self.setAl)\n",
        "        #for fixed alpha\n",
        "        #self.alpha =  tf.Variable(initial_value=self.init, name=self.nameU, constraint=keras.constraints.NonNeg(), dtype=tf.float32, trainable=False)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "\n",
        "        return config\n",
        "\n",
        "    def call(self, lam):\n",
        "\n",
        "        one = tf.constant(1.0,dtype='float32')\n",
        "        two = tf.constant(2.0,dtype='float32')\n",
        "        four = tf.constant(4.0,dtype='float32')\n",
        "\n",
        "\n",
        "        lam1_uc=lam[0]\n",
        "        lam1_ut=lam[1]\n",
        "        gamma=lam[2]\n",
        "\n",
        "        #for trainable alpha - set minus for negative sign\n",
        "        alp = -self.alpha\n",
        "        #for fixed alpha\n",
        "        #alph=2.0\n",
        "\n",
        "        lam1=lam1_uc\n",
        "        lam2=tf.math.pow(lam1,-0.5)\n",
        "        lam3=lam2\n",
        "        J1_uc=tf.math.pow(lam1,alp) + tf.math.pow(lam2,alp) + tf.math.pow(lam3,alp)\n",
        "        dJ1dF1_uc=tf.math.multiply(alp,tf.math.pow(lam1,alp))\n",
        "        minus_uc=tf.math.multiply(alp,tf.math.pow(lam3,alp))\n",
        "\n",
        "        lam1=lam1_ut\n",
        "        lam2=tf.math.pow(lam1,-0.5)\n",
        "        lam3=lam2\n",
        "        J1_ut=tf.math.pow(lam1,alp) + tf.math.pow(lam2,alp) + tf.math.pow(lam3,alp)\n",
        "        dJ1dF1_ut=tf.math.multiply(alp,tf.math.pow(lam1,alp))\n",
        "        minus_ut=tf.math.multiply(alp,tf.math.pow(lam3,alp))\n",
        "\n",
        "\n",
        "        lam1_ss=(gamma/2. + tf.math.sqrt(1.+(gamma**2/4.)))\n",
        "        lam2_ss=(-gamma/2. + tf.math.sqrt(1.+(gamma**2/4.)))\n",
        "        J1_ss=lam1_ss+lam2_ss + one\n",
        "        dJ1dF_ss=2**(-alp)*alp*(-(-gamma + tf.math.sqrt(gamma**2 + 4))**alp + (gamma + tf.math.sqrt(gamma**2 + 4))**alp)/tf.math.sqrt(gamma**2 + 4)\n",
        "\n",
        "\n",
        "        return [J1_uc, dJ1dF1_uc,minus_uc,J1_ut,dJ1dF1_ut,minus_ut,J1_ss,dJ1dF_ss]\n",
        "\n",
        "\n",
        "\n",
        "def modelArchitecture(Psi_model):\n",
        "    # Stretch and Gamma as input\n",
        "    Stretch1_uc = keras.layers.Input(shape = (1,),\n",
        "                                  name = 'Stretch1_uc')\n",
        "    Stretch1_ut = keras.layers.Input(shape = (1,),\n",
        "                                  name = 'Stretch1_ut')\n",
        "    Gamma = keras.layers.Input(shape = (1,),\n",
        "                                  name = 'Gamma')\n",
        "\n",
        "    J1_uc, dJ1dF1_uc,minus_uc,J1_ut,dJ1dF1_ut,minus_ut,J1_ss,dJ1dF_ss = H_Layer_generalized_combi('alpha1',True, 5.0)([Stretch1_uc,Stretch1_ut,Gamma])\n",
        "\n",
        "\n",
        "    # specific Invariants PS\n",
        "\n",
        "    I1_uc = keras.layers.Lambda(lambda x: x)(J1_uc)\n",
        "    I1_ut = keras.layers.Lambda(lambda x: x)(J1_ut)\n",
        "    I1_ss = keras.layers.Lambda(lambda x: x)(J1_ss)\n",
        "\n",
        "    # load specific models\n",
        "\n",
        "    Psi_uc = Psi_model([I1_uc])\n",
        "    Psi_ut = Psi_model([I1_ut])\n",
        "    Psi_ss = Psi_model([I1_ss])\n",
        "\n",
        "    # derivative PS\n",
        "    dWI1_uc  = keras.layers.Lambda(lambda x: myGradient(x[0], x[1]))([Psi_uc, I1_uc])\n",
        "    dWI1_ut  = keras.layers.Lambda(lambda x: myGradient(x[0], x[1]))([Psi_ut, I1_ut])\n",
        "    dWI1_ss  = keras.layers.Lambda(lambda x: myGradient(x[0], x[1]))([Psi_ss, I1_ss])\n",
        "\n",
        "    # Stress\n",
        "    Stress_11_uc = keras.layers.Lambda(function = Stress_cal_uni_11,\n",
        "                                 name = 'Stress_11_uc')([dWI1_uc,dJ1dF1_uc,minus_uc])\n",
        "    Stress_11_ut = keras.layers.Lambda(function = Stress_cal_uni_11,\n",
        "                                 name = 'Stress_11_ut')([dWI1_ut,dJ1dF1_ut,minus_ut])\n",
        "    Stress_ss = keras.layers.Lambda(function = Stress_cal_ss,\n",
        "                                 name = 'Stress_ss_12')([dWI1_ss,dJ1dF_ss])\n",
        "    # Define model for different load case\n",
        "    model_combi = keras.models.Model(inputs=[Stretch1_uc,Stretch1_ut,Gamma], outputs= [Stress_11_uc,Stress_11_ut,Stress_ss])\n",
        "\n",
        "    return model_combi, Psi_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saYHO1e24JMm"
      },
      "source": [
        "### 4. Compile model\n",
        "\n",
        "The compiler definition comprises the loss function definition (here a mean squared error metric), the optimizer (here an Adam optimizer) and the evaluation metric (also mean squared error).\n",
        "\n",
        "Moreover, we define model callbacks and the keras fit function. The latter obtains the information about which model we want to fit with which data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmCtDWke4J5Y"
      },
      "outputs": [],
      "source": [
        "# Optimization utilities\n",
        "def Compile_and_fit(model_given, input_train, output_train, epochs, path_checkpoint):\n",
        "\n",
        "    mse_loss = keras.losses.MeanSquaredError()\n",
        "    metrics  =[keras.metrics.MeanSquaredError()]\n",
        "    opti1    = tf.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    model_given.compile(loss=mse_loss,\n",
        "                  optimizer=opti1,\n",
        "                  metrics=metrics)\n",
        "\n",
        "    # if training loss starts to increase, stop training after 3000 additional epochs = \"patience\"\n",
        "    ## early stopping if loss increasing innstead of decreasing\n",
        "\n",
        "    es_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=0, patience=3000, restore_best_weights=True)\n",
        "\n",
        "    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
        "    monitor=\"loss\",\n",
        "    filepath=path_checkpoint,\n",
        "    verbose=0,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True, # save only the best weights across all epochs\n",
        "    )\n",
        "\n",
        "## model.fit - input/output pairs of data to train on\n",
        "    history = model_given.fit(input_train,\n",
        "                        output_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_split=0.0,\n",
        "                        callbacks=[es_callback, modelckpt_callback], # save best weights if stop early or go through all epochs\n",
        "                        shuffle = True,\n",
        "                        verbose = 0 ) # verbose = 2 will print loss each epoch\n",
        "\n",
        "\n",
        "    return model_given, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwHIjb4g4Pni"
      },
      "source": [
        "### 5. Plot functions\n",
        "\n",
        "Here we define some plot functions to be used to plot the results later on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yT9Nfkj4QaD"
      },
      "outputs": [],
      "source": [
        "def plotLoss(axe, history):\n",
        "    axe.plot(history)\n",
        "    axe.set_yscale('log')\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOYoQXHk4XfL"
      },
      "outputs": [],
      "source": [
        "# plot the contribution of each term to the model stress prediction\n",
        "\n",
        "def color_map(ax, stretch, model, model_weights, Psi_model, cmaplist, terms, model_type, mode):\n",
        "    print(mode)\n",
        "    if mode=='c':\n",
        "      j=0\n",
        "    elif mode=='t':\n",
        "      j=1\n",
        "    elif mode=='s':\n",
        "      j=2\n",
        "    print(j)\n",
        "    stretch_triple=stretch\n",
        "    stretch=stretch[j]\n",
        "    predictions = np.zeros([stretch.shape[0], terms])\n",
        "    model_plot = copy.deepcopy(model_weights)  # deep copy model weights\n",
        "\n",
        "    for i in range(terms):\n",
        "        if model_type == 'Stretch':\n",
        "            model_plot = np.zeros_like(model_weights)  # wx1 all set to zero\n",
        "            model_plot[i] = model_weights[i]  # wx1[i] set to trained value\n",
        "        else:  # for architectures with multiple layers (VL, invariant)\n",
        "            model_plot[-1] = np.zeros_like(model_weights[-1])  # wx2 all set to zero\n",
        "            model_plot[-1][i] = model_weights[-1][i]  # wx2[i] set to trained value\n",
        "\n",
        "        Psi_model.set_weights(model_plot)\n",
        "        lower = np.sum(predictions, axis=1)\n",
        "        upper = lower + model.predict(stretch_triple, verbose=0)[j].flatten()\n",
        "        predictions[:, i] = model.predict(stretch_triple, verbose=0)[j].flatten()\n",
        "        ax.fill_between(stretch[:], lower.flatten(), upper.flatten(), lw=0, zorder=i + 1, color=cmaplist[i],\n",
        "                         label=i + 1)\n",
        "        ## plot each term's contribution\n",
        "        ax.plot(stretch, upper, lw=0.4, zorder=34, color='k')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt5Fokjg4ags"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['xtick.major.pad'] = 14 # set plotting parameters\n",
        "plt.rcParams['ytick.major.pad'] = 14\n",
        "\n",
        "def generate_blue_shades(n=20):\n",
        "    blue_shades = []\n",
        "    for i in range(n):\n",
        "        t = i / (n - 1)\n",
        "\n",
        "        # Blue from 50 → 255\n",
        "        b = int(50 + 205 * t)\n",
        "\n",
        "        if t <= 0.5:\n",
        "            # Dark to classical blue: keep R and G at 0\n",
        "            r = g = 0\n",
        "        else:\n",
        "            # Pale blue: increase green moderately, red minimally\n",
        "            fade = (t - 0.5) * 2  # 0 → 1 over second half\n",
        "            g = int(fade * 200)   # green adds brightness\n",
        "            r = int(fade * 80)    # red is kept low to avoid violet\n",
        "\n",
        "        hex_color = f\"#{r:02X}{g:02X}{b:02X}\"\n",
        "        blue_shades.append(hex_color)\n",
        "    return blue_shades\n",
        "\n",
        "\n",
        "def generate_red_shades(n=20):\n",
        "    red_shades = []\n",
        "    for i in range(n):\n",
        "        # Position as fraction of the range\n",
        "        t = i / (n - 1)\n",
        "\n",
        "        # Red from 50 → 255 uniformly\n",
        "        r = int(50 + 205 * t)\n",
        "\n",
        "        if t <= 0.5:\n",
        "            # Dark to pure red: green/blue = 0\n",
        "            g = b = 0\n",
        "        else:\n",
        "            # From pure red to pale red: increase G/B gradually to 230 max\n",
        "            fade = (t - 0.5) * 2  # scale from 0 → 1 in second half\n",
        "            gb = int(fade * 230)\n",
        "            g = b = gb\n",
        "\n",
        "        hex_color = f\"#{r:02X}{g:02X}{b:02X}\"\n",
        "        red_shades.append(hex_color)\n",
        "    return red_shades\n",
        "\n",
        "\n",
        "# plot tension, compression, and shear brain data with color maps\n",
        "\n",
        "def plotMapAll(ax, Psi_model, model_weights, model_given, terms, lam_ut, P_ut, Region, path2saveResults, modelFit_mode, model_type):\n",
        "    blue_list = generate_blue_shades(30)\n",
        "    red_list = generate_red_shades(30)\n",
        "    cmaplist = [blue_list[15],blue_list[15],blue_list[15]]\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    fig, ax = plt.subplots(figsize=(12.5, 8.33))\n",
        "    ax.set_xlim(1.00,0.9)\n",
        "    ax.set_ylim( 0.01,-1.05)\n",
        "    color_map(ax, lam_ut, model_given, model_weights, Psi_model, cmaplist, terms, model_type,'c')\n",
        "    ax.scatter(lam_ut[0], P_ut[0], s=800, zorder=103, lw=3, facecolors='w', edgecolors='k', clip_on=False)\n",
        "    plt.tight_layout(pad=2)\n",
        "    plt.savefig(path2saveResults + '/uc_' + 'Train'+ modelFit_mode + '_' + 'Region' + Region + '.pdf')\n",
        "    fig, ax = plt.subplots(figsize=(12.5, 8.33))\n",
        "    ax.set_xlim(1., 1.1)\n",
        "    ax.set_ylim(0., 0.456)\n",
        "    color_map(ax, lam_ut, model_given, model_weights, Psi_model, cmaplist, terms, model_type,'t')\n",
        "    ax.scatter(lam_ut[1], P_ut[1], s=800, zorder=103, lw=3, facecolors='w', edgecolors='k', clip_on=False)\n",
        "    plt.tight_layout(pad=2)\n",
        "    plt.savefig(path2saveResults + '/ut_' + 'Train'+ modelFit_mode + '_' + 'Region' + Region + '.pdf')\n",
        "    fig, ax = plt.subplots(figsize=(12.5, 8.33))\n",
        "    color_map(ax, lam_ut, model_given, model_weights, Psi_model, cmaplist, terms, model_type,'s')\n",
        "    ax.scatter(lam_ut[2], P_ut[2], s=800, zorder=103, lw=3, facecolors='w', edgecolors='k', clip_on=False)\n",
        "    plt.tight_layout(pad=2)\n",
        "    ax.set_xlim(-0.00, 0.20)\n",
        "    ax.set_ylim(-0.001, 0.5438)\n",
        "    plt.savefig(path2saveResults + '/ss_' + 'Train'+ modelFit_mode + '_' + 'Region' + Region + '.pdf')\n",
        "    #plt.close();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVRbZjO_4jBw"
      },
      "source": [
        "### 6. Model Training\n",
        "\n",
        "Parameters and definitions for the model training. Try changing the number of epochs and toggling between the invariant and principal-stretch-based model. Make sure to rename the model_type variable for each test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqeeGfNH4iNq"
      },
      "outputs": [],
      "source": [
        "train = True\n",
        "epochs = 30000# try ~30,000 epochs for a good fit\n",
        "batch_size = 12\n",
        "folder_name = 'brain-uc-ut-ss-reg01-one-inv-negative-with-ln' # name the folder for your results\n",
        "\n",
        "### Choose regularization type & penalty amount\n",
        "# Option: 'L1', 'L2'; for VL used L2=0.001, for Stretch no reg was used\n",
        "reg = 'L1'\n",
        "pen = 0.01  # Use 0 for no regularization\n",
        "\n",
        "### Choose which model type to build CANN architecture with\n",
        "# Options: 'Stretch', 'Invariant'\n",
        "# 'Stretch' is principal-stretch-based and contains stretches raised to fixed powers (range & number of terms can be adjusted)\n",
        "# 'Invariant' is invariant-based and contains I2, I2, I1^2, I2^2 and all with exp() and ln() activations\n",
        "model_type = 'Invariant'\n",
        "\n",
        "### Choose which loading modes to train with\n",
        "modelFit_mode_all = ['combi']\n",
        "\n",
        "### Choose which types of material to train with\n",
        "Region_all = ['brain']\n",
        "################################################\n",
        "\n",
        "\n",
        "def makeDIR(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "filename = 'brain-train-uc-ut-test-ss'\n",
        "path2saveResults_0 = path + 'results/'+filename+'/'+folder_name\n",
        "makeDIR(path2saveResults_0)\n",
        "Model_summary = path2saveResults_0 + '/Model_summary.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qYkzZBq_PG1r"
      },
      "outputs": [],
      "source": [
        "#  Training and validation loop\n",
        "count = 1\n",
        "for id1, Region in enumerate(Region_all): # loop through brain region data\n",
        "\n",
        "    #R2_all_Regions = []\n",
        "    for id2, modelFit_mode in enumerate(modelFit_mode_all): # loop through model training modes\n",
        "\n",
        "        print(40*'=')\n",
        "        print(\"Comp {:d} / {:d}\".format(count, len(Region_all)*len(modelFit_mode_all)))\n",
        "        print(40*'=')\n",
        "        print(\"Region: \", Region ,\"| Fitting Mode: \", modelFit_mode)\n",
        "        print(40*'=')\n",
        "        count += 1\n",
        "\n",
        "        path2saveResults = os.path.join(path2saveResults_0,Region, modelFit_mode)\n",
        "        path2saveResults_check = os.path.join(path2saveResults,'Checkpoints')\n",
        "        makeDIR(path2saveResults)\n",
        "        makeDIR(path2saveResults_check)\n",
        "\n",
        "        # load experimantal data\n",
        "        lam1_ut,T1_ut,lam1_uc,T1_uc, lam_ss, T1_ss = getStressStrain(Region) # stress/stretch/shear pairs\n",
        "\n",
        "        # Model selection\n",
        "        if model_type == 'Invariant':\n",
        "            Psi_model, terms = StrainEnergyCANN_invariant(reg, pen) # build invariant-based model\n",
        "        elif model_type == 'Stretch':\n",
        "            Psi_model, terms = StrainEnergyCANN_stretch(reg, pen) # build principle-stretch-based model\n",
        "        model_combi, Psi_model = modelArchitecture(Psi_model) # build uniaxial and shear models\n",
        "\n",
        "\n",
        "        with open(Model_summary,'w') as fh:\n",
        "            # Pass the file handle in as a lambda function to make it callable\n",
        "            Psi_model.summary(line_length=80, print_fn=lambda x: fh.write(x + '\\n')) # summarize layers in architecture\n",
        "\n",
        "        #%%  Model training\n",
        "        model_given, input_train, output_train = traindata(modelFit_mode) # model type, input/output pairs\n",
        "\n",
        "\n",
        "        Save_path = path2saveResults + '/model.h5'\n",
        "        Save_weights = path2saveResults + '/weights'\n",
        "        path_checkpoint = path2saveResults_check + '/best_weights'\n",
        "        if train: # use compile/fit parameters to train specific model (UT, SS, both) with specific input/output pairs\n",
        "            model_given, history = Compile_and_fit(model_given, input_train, output_train, epochs, path_checkpoint)\n",
        "\n",
        "            model_given.load_weights(path_checkpoint, by_name=False, skip_mismatch=False) # load the weights saved in the path (the best ones)\n",
        "            tf.keras.models.save_model(Psi_model, Save_path, overwrite=True) # save the model\n",
        "            Psi_model.save_weights(Save_weights, overwrite=True) # save the weights\n",
        "\n",
        "            # Plot loss function\n",
        "            loss_history = history.history['loss']\n",
        "            fig, axe = plt.subplots(figsize=[6, 5])  # inches\n",
        "            plotLoss(axe, loss_history)\n",
        "            plt.savefig(path2saveResults+'/Plot_loss_'+Region+'_'+modelFit_mode+'.pdf')\n",
        "            plt.show()\n",
        "            #plt.close()\n",
        "\n",
        "        else: # if already trained, just load the saved weights\n",
        "            Psi_model.load_weights(Save_weights, by_name=False, skip_mismatch=False)\n",
        "\n",
        "\n",
        "        # Show weights\n",
        "        if model_type == 'Stretch':\n",
        "            weight_matrix = np.empty((terms, 1))\n",
        "            for i in range(terms):\n",
        "                value = Psi_model.get_weights()[i]\n",
        "                weight_matrix[i] = value\n",
        "            print(\"weight_matrix\")\n",
        "            print(*weight_matrix, sep=\"\\n\")\n",
        "\n",
        "        elif model_type == 'Invariant':\n",
        "            weight_matrix = np.empty((terms, 2))\n",
        "            for i in range(terms):\n",
        "                value = Psi_model.get_weights()[i][0][0]\n",
        "                weight_matrix[i, 0] = value\n",
        "                weight_matrix[:, 1] = Psi_model.get_layer('wx2').get_weights()[0].flatten()\n",
        "            print(\"weight_matrix\")\n",
        "            print(weight_matrix)\n",
        "\n",
        "        # Get the trained weights\n",
        "        model_weights_0 = Psi_model.get_weights()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M6czOgrrkrdk"
      },
      "outputs": [],
      "source": [
        "# assign simple shear values for testing\n",
        "lam_ss = dfs.iloc[19:36,2].dropna().astype(np.float64).values\n",
        "T1_ss = dfs.iloc[19:36,3].dropna().astype(np.float64).values\n",
        "\n",
        "Stress_predict_combi = model_given.predict([lam1_uc,lam1_ut,lam_ss], verbose=0)\n",
        "\n",
        "# plot the contributions of each term to the output of the model\n",
        "fig, ax = plt.subplots(figsize=(12.5, 8.33))\n",
        "plotMapAll(ax,Psi_model, model_weights_0, model_given, terms, [lam1_uc,lam1_ut,lam_ss],[T1_uc,T1_ut,T1_ss], Region, path2saveResults, modelFit_mode, model_type)\n",
        "\n",
        "R2_uc = r2_score(T1_uc,Stress_predict_combi[0])\n",
        "R2_ut= r2_score(T1_ut, Stress_predict_combi[1])\n",
        "R2_ss = r2_score(T1_ss, Stress_predict_combi[2])\n",
        "\n",
        "print('R2_uc = ', R2_uc)\n",
        "print('R2_ut = ', R2_ut)\n",
        "print('R2_ss = ', R2_ss)\n",
        "\n",
        "a1=model_given.get_weights()[0]\n",
        "a2=model_given.get_weights()[1]\n",
        "\n",
        "#Save trained weights and R2 values to txt file\n",
        "Config = {\"Region\": Region, \"modelFit_mode\": modelFit_mode, 'model_type': model_type, 'Reg': reg, 'Penalty': pen, \"R2_uc\": R2_uc, \"R2_ut\": R2_ut, \"R2_ss\": R2_ss,\n",
        "                      \"alpha_pos\": a1.tolist(),\"alpha_neg\": a2.tolist(),\"weights\": weight_matrix.tolist()}\n",
        "json.dump(Config, open(path2saveResults + \"/Config_file.txt\", 'w'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UZ2NVEtlXvnm",
        "outputId": "083e874e-9d5f-48a9-e1c5-9afbe0d70f19"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_blue_shades(n=20):\n",
        "    \"\"\"\n",
        "    Generates a list of 'n' blue shades from dark to pale blue (near white),\n",
        "    passing through classical blue in the middle, and avoids violet tinting\n",
        "    by keeping red low and green moderate.\n",
        "    \"\"\"\n",
        "    blue_shades = []\n",
        "    for i in range(n):\n",
        "        t = i / (n - 1)\n",
        "\n",
        "        # Blue from 50 → 255\n",
        "        b = int(50 + 205 * t)\n",
        "\n",
        "        if t <= 0.5:\n",
        "            # Dark to classical blue: keep R and G at 0\n",
        "            r = g = 0\n",
        "        else:\n",
        "            # Pale blue: increase green moderately, red minimally\n",
        "            fade = (t - 0.5) * 2  # 0 → 1 over second half\n",
        "            g = int(fade * 200)   # green adds brightness\n",
        "            r = int(fade * 80)    # red is kept low to avoid violet\n",
        "\n",
        "        hex_color = f\"#{r:02X}{g:02X}{b:02X}\"\n",
        "        blue_shades.append(hex_color)\n",
        "    return blue_shades\n",
        "\n",
        "# Convert hex to RGB (0–1 range for matplotlib)\n",
        "def hex_to_rgb01(hex_color):\n",
        "    hex_color = hex_color.lstrip(\"#\")\n",
        "    r = int(hex_color[0:2], 16) / 255\n",
        "    g = int(hex_color[2:4], 16) / 255\n",
        "    b = int(hex_color[4:6], 16) / 255\n",
        "    return (r, g, b)\n",
        "\n",
        "def generate_red_shades(n=20):\n",
        "    \"\"\"\n",
        "    Generates a list of 'n' red shades from dark red to light red (near white),\n",
        "    passing through a true, pure red in the middle.\n",
        "    Green and blue stay at 0 in the first half to preserve classical red.\n",
        "    \"\"\"\n",
        "    red_shades = []\n",
        "    for i in range(n):\n",
        "        # Position as fraction of the range\n",
        "        t = i / (n - 1)\n",
        "\n",
        "        # Red from 50 → 255 uniformly\n",
        "        r = int(50 + 205 * t)\n",
        "\n",
        "        if t <= 0.5:\n",
        "            # Dark to pure red: green/blue = 0\n",
        "            g = b = 0\n",
        "        else:\n",
        "            # From pure red to pale red: increase G/B gradually to 230 max\n",
        "            fade = (t - 0.5) * 2  # scale from 0 → 1 in second half\n",
        "            gb = int(fade * 230)\n",
        "            g = b = gb\n",
        "\n",
        "        hex_color = f\"#{r:02X}{g:02X}{b:02X}\"\n",
        "        red_shades.append(hex_color)\n",
        "    return red_shades\n",
        "\n",
        "\n",
        "# Generate color list and convert\n",
        "colors = generate_blue_shades(20)\n",
        "rgb_colors = [hex_to_rgb01(c) for c in colors]\n",
        "\n",
        "# Create gradient by interpolating\n",
        "gradient = np.linspace(0, 1, 500).reshape(1, -1)  # 1 row, 500 columns\n",
        "gradient_colors = np.zeros((1, 500, 3))           # RGB image\n",
        "\n",
        "# Interpolate manually between the blue shades\n",
        "steps = len(rgb_colors) - 1\n",
        "for i in range(500):\n",
        "    t = i / 499 * steps\n",
        "    idx = int(t)\n",
        "    frac = t - idx\n",
        "    if idx < steps:\n",
        "        c1 = np.array(rgb_colors[idx])\n",
        "        c2 = np.array(rgb_colors[idx + 1])\n",
        "        color = (1 - frac) * c1 + frac * c2\n",
        "    else:\n",
        "        color = rgb_colors[-1]\n",
        "    gradient_colors[0, i] = color\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(10, 1))\n",
        "plt.imshow(gradient_colors, aspect='auto')\n",
        "plt.axis('off')\n",
        "plt.title(\"Smooth Gradient of Blue Shades\", pad=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}